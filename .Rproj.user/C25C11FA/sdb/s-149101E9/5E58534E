{
    "collab_server" : "",
    "contents" : "---\ntitle: \"p8105_hw3_ym2649\"\nauthor: \"Yutian Mu\"\ndate: \"10/4/2017\"\noutput: html_document\n---\n\n```{r load_libraries}\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(haven)\nlibrary(ggridges)\nlibrary(ggthemes)\nlibrary(readr)\nlibrary(rnoaa)\n```\n\n```{r figure_size}\nknitr::opts_chunk$set(\n  fig.width = 6,\n  fig.asp = .6,\n  out.width = \"90%\"\n)\n```\n\n#Problem1\n```{r clean_pulse}\npulse_data = read_sas(\"../data/public_pulse_data.sas7bdat\") %>%\nclean_names()\npulse_tidy = gather(pulse_data, key = visit, value = bdi, bdiscore_bl:bdiscore_12m) %>%\nfilter(!is.na(bdi))\n```\n\n*Briefly summarize the dataset. How many subjects are included? Make a table showing the number of subjects with observations at 1, 2, 3, or 4 visits.\n\nThere are **`r nrow(pulse_tidy)`** subjects included.\n```{r n_subjects}\npulse_number<-pulse_tidy %>%\nseparate(visit, into = c(\"remove\", \"visit\"), sep = \"_\") %>%\n   select(-remove)%>% \n   mutate(visit = replace(visit, visit == \"bl\", \"00m\"),\n         visit = factor(visit, levels = paste0(c(\"00\", \"01\", \"06\", \"12\"), \"m\")),\n         visit = as.numeric(factor(visit))) \n\npulse_number %>%\n  group_by(visit) %>%\n  summarize(n_subjects = n())\n```\n\n*Make a table showing showing the mean, median, and standard deviaion of the BDI score at each visit.\n```{r sum_pulse}\npulse_number%>%\n  group_by(visit) %>%\n  summarize(mean_bdi=mean(bdi),\n            median_bdi=median(bdi),\n            sd_bdi=sd(bdi))\n```            \n\n*Make box and violin plots showing the distribution of BDI score at each visit. Comment on the distribution of BDI score.\n```{r pulse_figure}\npulse_tidy %>%\n  mutate(visit = forcats::fct_relevel(visit, c(\"bdiscore_bl\", \"bdiscore_01m\", \"bdiscore_06m\",\"bdiscore_12m\"))) %>% \n  ggplot(aes(x = visit, y = bdi)) + geom_boxplot()+\n  labs(title=\"Distribution of BDI score at each visit\",y=\"BDI score\")\n\npulse_tidy%>%\n  mutate(visit = forcats::fct_relevel(visit, c(\"bdiscore_bl\", \"bdiscore_01m\", \"bdiscore_06m\",\"bdiscore_12m\"))) %>% \n  ggplot(aes(x = visit, y = bdi)) + \n     geom_violin(aes(fill = visit), color = \"blue\", alpha = .5) + \n     stat_summary(fun.y = median, geom = \"point\", color = \"blue\", size = 4) +\n     theme(legend.position = \"bottom\")+labs(title=\"Distribution of BDI score at each visit\",y=\"BDI score\")\n```\n\n\nThe boxplot shows that the BDI score in the second, third and forth visits have very similar median(about 4), and very similar Q1(about 1).The Q3 of BDI score in the second and forth visits are around 9, and that of the third visit is aroud 8. The BDI score in the first visit has larger Q1, median and Q3 compared with other three visits. And there are many outliers above Q3 in all four groups.\n\nFrom the violin plot, we can have more intuitional idea about the bdi score distribution. The BDI score in the last three groups has similar distribution. The last three plots concentrate on smaller values (between 0 and 15), heavily around 0 and turn narrower as the score gets larger. The first group is relatively narrower on 0, but the overall distribution is similar with the others. All four groups have many outliers from aroud 25 to 50.\n\n\n*Covert visit to a numeric variable, and make a “speghetti plot” of BDI score. That is, make a scatterplot showing each subject’s BDI score at each visit with lines connected repeated observations on the same subject. geom_path() will be useful. Based on this plot, comment on the stability of BDI score within a person over time – do subjects with high BDI scores at baseline tend to have high BDI scores at 12 months?\n\n```{r pulse_connect}\npulse_number<-pulse_tidy %>%\nseparate(visit, into = c(\"remove\", \"visit\"), sep = \"_\") %>%\n  select(-remove) %>% \n  mutate(visit = replace(visit, visit == \"bl\", \"00m\"),\n         visit = factor(visit, levels = paste0(c(\"00\", \"01\", \"06\", \"12\"), \"m\")),\n         visit = as.numeric(factor(visit)))\n\npulse_number%>%  \nggplot(aes(x = visit, y = bdi, color=visit)) + \n      geom_point() + geom_path(aes(group=id))+labs(title=\"Distribution of BDI score for each subject at each visit\",y=\"BDI score\")\n```\n\n\nThe BDI score within a person over time is unstable, since we can see many broken lines from the plot. And the subjects with high BDI scores at baseline can have low bdi scores at 12 months. We cannot see any trends.\n\n\n#problem2\n```{r read_instacart}\ninstacart= read_csv(\"../data/instacart_train_data.csv.zip\",col_types=\"iiiiiciiiiciicc\")\n```\n\n*Produce a table showing how many items are ordered in each department, and limit this table to the seven departments from which the most items are ordered.\ninstacart %>%\n  group_by(department) %>%\n  summarize(items_ordered = n()) %>%\n  mutate(depart_rank = min_rank(desc(items_ordered))) %>%\n  filter(depart_rank <= 7) %>%\n  arrange(depart_rank)\n```{r order_department}\ninstacart %>%\n  group_by(department) %>%\n  summarize(n_items = n_distinct(product_name)) %>%\n  filter(min_rank(desc(n_items)) < 8)\n```  \n\n*Make a table showing the most popular item in each department.\n```{r popular_item}\ninstacart %>%\n  group_by(department, product_name) %>%\n  summarize(n_product=n())%>%\n  filter(min_rank(desc(n_product)) ==1)\n```\n\n*Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).\n```{r mean_hour}\ninstacart %>%\n  group_by(product_name,order_dow) %>%\n  summarize(mean_hour_of_day=mean(order_hour_of_day)) %>%\n  filter(product_name==\"Pink Lady Apples\"|product_name ==\"Coffee Ice Cream\") %>%\n  spread(order_dow,mean_hour_of_day)\n```\n\n*Using violin and ridge plots, show the distribution of the order hour of the day for each department. Organize your plot according to the IQR of the order hour. Comment on the distributions, taking into account the which departments have the widest and narrowest IQRs.\n```{r instacart_figure}\ninstacart%>%\nmutate(name = forcats::fct_reorder(department, order_hour_of_day, IQR))%>%\n  ggplot(aes(x = name, y = order_hour_of_day)) + \n  geom_violin(aes(fill = name), color = \"blue\", alpha = .5) +\n  stat_summary(fun.y = median, geom = \"point\", color = \"blue\", size = 2)+\n  theme(legend.position = \"bottom\",axis.text.x = element_text(angle = 45,, vjust=0.5, hjust=0.5))+labs(title=\"Distribution of the order hour of the day for each department\",x=\"department\")\n\ninstacart%>%\nmutate(name = forcats::fct_reorder(department, order_hour_of_day, IQR))%>%\nggplot(aes(x = order_hour_of_day, y = name)) + \n  geom_density_ridges(scale = .85)+\n  labs(y=\"department\")+labs(title=\"Distribution of order hour of the day for each department\",y=\"department\")\n```\n\nThe violin plot shows that all plots are wider in the middle of distribution (approximatly from 10 to 15). For some groups (like alcohol,deli,dry goods pasta,personal care and so on) the plot has similar density from 10 to 15, and in other groups, it is more randomly distributed.\n\nThe ridge plot shows that the hour of the day on which the order was placed has very similar distribution among different departments, with higher density on approximately from 7.5 to 20.\n\n\"alcohol\" and \"canned goods\" have the narrowest IQRs--5.\"babies\",\"\tbakery\",\"breakfast\",\"\tdairy eggs\",\"household\", and \"personal care\" have the widest IQRs--7.\n\n\n#problem3\n```{r clean_nynoaa}\nnynoaa=read_csv(\"../data/nynoaadat.zip\",col_types=\"cDiiicc\") %>%\nseparate(col= date, into = c(\"year\", \"month\", \"day\"), sep = \"-\")\n```\n\n*Write a short description of the dataset. How many observations are included? How many stations? How much missing data is there for tmax and snow? Does this vary by station?\n\nThe dataset contains weather information including snowfall and temperature from weather stations around the world. There are **`r nrow(nynoaa)`** observations of 7 variables in the dataset.\n\nThe number of stations:\n```{r n_stations}\nnynoaa %>%\ngroup_by(id) %>%\nsummarize(n=n())%>%\nnrow()\n```\n\nThe number of missing data of each station:\n```{r n_missing}  \nnynoaa %>%\ngroup_by(id) %>%\nsummarize(tmax_NA=sum(is.na(tmax)))\n\nnynoaa %>%\ngroup_by(id) %>%\nsummarize(snow_NA=sum(is.na(snow)))\n```\n\nThere are **`r sum(is.na(nynoaa$tmax))`** missing data for tmax, and **`r sum(is.na(nynoaa$snow))`** for snow.And from the table above we can see that the missing data vary by station.\n\n\n*What is the year that contains the largest snowfall on a single day at any single weather station (i.e. what is the year containing the single largest snowfall anywhere in the state in the 30 years of monitoring). Can you find information online that supports your finding?\n```{r nynoaa_snowfall}\nnynoaa %>%\n  filter(!is.na(snow))%>%\n  filter(min_rank(desc(snow))==1)\n```\nSo year 1983 had the largest snowfall at 11 April 1983.\nFrom information online we know that the weather station USC00309516 is East Windham New York. But I cannot find information about snowfall on that specific day online.\n\n*Limiting your data to observations with snowfall values greater than 0 and less than 100, make a ridge plot showing the distribution of snowfall values for each year. Comment on the recorded snowfall values – are they clustered around specific entries? If so, why?\n```{r nynoaa_ridge}\nnynoaa %>%\nfilter(snow>0 & snow< 100) %>%\nggplot(aes(x = snow, y = year)) + \n     geom_density_ridges(scale = .85)+labs(title=\"Distribution of snowfall values for each year\")\n```\n\nFrom ridge plot, we can see that the snowfall value has very similar distribution among different years with two peaks at about 15 and 30, and also a small peak at about 50.They are clustered around 15,30 and 50.\n\n\n*Make a useful plot showing tmax against tmin. For these data, you might try a scatterplot, but it is unlikely to be “useful”.\n```{r tmax_tmin}\nnynoaa %>%\n  group_by(year) %>%\n  filter(!is.na(tmax)&!is.na(tmin))%>%\n  mutate(tmax=as.numeric(tmax),tmin=as.numeric(tmin))%>%\n  mutate(diff = tmax - tmin) %>% \n  \nggplot(aes(x = year, y = diff, color =year)) + \n  geom_violin(aes(fill = year), color = \"blue\", alpha = .5)+\n    stat_summary(fun.y = median, geom = \"point\", color = \"blue\", size = 1)+\n    theme(legend.position = \"bottom\",axis.text.x = element_text(angle = 45))+labs(title=\"Distribution of tmax against tmin over years\",y=\"tmax-tmin\")\n```\n\n\n*Separate the date variable into year, month, and day variables. For each station and month, average across year to obtain the station-specific monthly average tmax. Make a spaghetti plot showing the average tmax curve for each station. Comment on your plot.\n\n```{r station_month}\nnynoaa %>%\n  mutate(year = as.numeric(year), month = as.numeric(month), day = as.numeric(day)) %>% \n  mutate(tmax=as.numeric(tmax))%>%\n  group_by(id,month)%>%\n  summarize(mean_tmax = mean(tmax, na.rm = TRUE))%>% \n  filter(!is.na(mean_tmax)) %>% \n  \n  ggplot(aes(x = month, y = mean_tmax,group=id,color=id))+geom_path()+ theme(legend.position = \"none\") + labs(title = \"Distribution of monthly tmax for each station\")\n```\n\nThe plot shows the distributions of the station-specific monthly average over years. Different stations have different values but the overall trends are similar. The plots go up from Jaunuary and peak at about July with mean_tmax from approximately 220 to 300 and then go down. And the plots are roughly symmetrical by July.\n",
    "created" : 1510070336501.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "868036195",
    "id" : "5E58534E",
    "lastKnownWriteTime" : 1510070338,
    "last_content_update" : 1510072060695,
    "path" : "~/Documents/data_science/p8105_hw3_ym2649/p8105_hw3_ym2649.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}