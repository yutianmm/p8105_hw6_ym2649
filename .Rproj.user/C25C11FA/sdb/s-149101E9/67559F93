{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Homework 3 solutions\"\noutput:\n  html_document: \n    toc: true\n    toc_float: true\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n### Due date\n\nDue: October 13 at 5:00pm. \n\n### Points\n\n* Problem 0: 10 points\n* Problem 1: 30 points\n* Problem 2: 30 points\n* Problem 3: 30 points\n\n### Problem 0\n\nThis \"problem\" focuses on structure of your assignment, including the use of R Markdown to write reproducible reports, the use of R Projects to organize your work, the use of relative paths to load data, and the naming structure for your files. \n\nTo that end: \n\n* create a directory named `p8105_hw2_YOURUNI` (e.g. `p8105_h21_ajg2202` for Jeff)\n* put an R project in the directory\n* create a single .Rmd file named `p8105_hw2_YOURUNI.Rmd`\n\nSome of the datasets used in this homework are large, so you should not include raw data files in your directory. Instead, create a separate directory called `data` and use relative paths starting with `../data/` to load data. We'll have a similar directory and should be able to knit your R Markdown file. \n\n\n```{r}\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(ggridges)\nlibrary(janitor)\n```\n\n### Problem 1\n\nRead and clean the PULSE dataset; omit observations for which BDI score wasn’t measured. \n\nThe code below cleans the PULSE dataset and converts it from wide to long format. We also remove missing values for the BDI score variable.\n\n```{r clean_pulse}\npulse_data = read_sas(\"../data/public_pulse_data.sas7bdat\") %>%\n  clean_names() %>%\n  gather(key = visit, value = bdi, bdiscore_bl:bdiscore_12m) %>%\n  separate(visit, into = c(\"remove\", \"visit\"), sep = \"_\") %>%\n  select(id, visit, everything(), -remove) %>%\n  mutate(visit = replace(visit, visit == \"bl\", \"00m\"),\n         visit = factor(visit, levels = paste0(c(\"00\", \"01\", \"06\", \"12\"), \"m\"))) %>%\n  arrange(id, visit) %>% \n  filter(!is.na(bdi))\n```\n\n\n******\n\nBriefly summarize the dataset. How many subjects are included? Make a table showing the number of subjects with observations at 1, 2, 3, or 4 visits.\n\n\n```{r pulse_summary}\nvisits_table = pulse_data %>%\n  count(id) %>%\n  count(n) %>%\n  rename(num_visits = n, count = nn)\n\nvisits_table\n\n```\n\nThere are `r pulse_data %>% summarize(n_distinct(id))` subjects in the PULSE dataset. The table above shows that `r visits_table %>% filter(num_visits == 1) %>% select(count)` subjects have 1 visit, `r visits_table %>% filter(num_visits == 2) %>% select(count)` have two visits, `r visits_table %>% filter(num_visits == 3) %>% select(count)` have 3 visits, and `r visits_table %>% filter(num_visits == 4) %>% select(count)` have 4 visits.\n\n******\n\nMake a table showing showing the mean, median, and standard deviation of the BDI score at each visit.\n\n```{r pulse_stats}\npulse_data %>%\n  group_by(visit) %>%\n  summarize(mean_bdi = mean(bdi),\n            median_bdi = median(bdi),\n            sd_bdi = sd(bdi))\n\n```\n\nThe table above shows the mean, median, and standard deviation of the BDA score at each visit. BDI score tends to be highest at baseline.\n\n******\n\nMake box and violin plots showing the distribution of BDI score at each visit.\n\n```{r pulse_violin}\npulse_data %>%\n  ggplot(aes(x = visit, y = bdi)) + geom_boxplot()\n\npulse_data %>%\n  ggplot(aes(x = visit, y = bdi)) + geom_violin()\n```\n\nAt each visit BDI score tends to be low for most people but is right skewed, meaning that some people have a much higher score than average. Across visits BDI score tends to decrease from baseline to 6 months, then levels off or slightly increases from 6 months to 12 months. The shapes of the distributions are more similar to each other at 1 month, 6 months, and 12 months than at baseline.\n\n******\n\nConvert visit to a numeric variable, and make a “spaghetti plot” of BDI score.\n\n```{r pulse_spaghetti}\npulse_data %>%\n  separate(visit, into = c(\"visit\", \"remove\"), 2) %>%\n  mutate(visit = as.numeric(visit)) %>% \n  ggplot(aes(x = visit, y = bdi, group = id)) + \n    geom_point() +\n    geom_path(alpha = .25) \n```\n\nBDI score is quite variable within a subject. The plot above does seem to indicate that subjects with high scores at baseline have high scores at 12 months, though subjects with high BDI scores at baseline tend to be more variable throughout the followup period than subjects with low BDI scores at baseline.\n\n### Problem 2\n\nLoad the Instacart data.\n\n```{r read_instacart}\ninstacart = read_csv(\"../data/instacart_train_data.csv.zip\")\n```\n\n\nProduce a table showing how many items are ordered in each department; for this question, limit the table to the seven departments from which the most items are ordered. \n\nAs shown below, the produce department has the most items ordered, followed by dairy and eggs, and then by snacks.\n\n```{r items_ordered}\ninstacart %>%\n  group_by(department) %>%\n  summarize(items_ordered = n()) %>%\n  mutate(depart_rank = min_rank(desc(items_ordered))) %>%\n  filter(depart_rank <= 7) %>%\n  arrange(depart_rank)\n```\n\nMake a table showing the most popular item in each department. \n\nThe code below produces this table. People like Honey Nut Cheerios and and Sauvignon Blanc.\n\n```{r popular_items}\ninstacart %>% \n  group_by(department, product_name) %>%\n  summarize(items_ordered = n()) %>%\n  mutate(item_rank = min_rank(desc(items_ordered))) %>%\n  filter(item_rank < 2)\n```\n\n\nMake a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).\n\nCoffee Ice Cream is often ordered later in the day. \n\n```{r apples}\ninstacart %>%\n  filter(product_name %in% c(\"Pink Lady Apples\", \"Coffee Ice Cream\")) %>%\n  group_by(product_name, order_dow) %>%\n  summarize(mean_hour = mean(order_hour_of_day)) %>%\n  spread(key = order_dow, value = mean_hour) \n```\n\nUsing violin and ridge plots, show the distribution of the order hour of the day for each department. Organize your plot according to the IQR of the order hour. Comment on the distributions, taking into account the which departments have the widest and narrowest IQRs.\n\n```{r instacart_plots}\ninstacart %>%\n  mutate(department = forcats::fct_reorder(department, order_hour_of_day, IQR)) %>%\n  ggplot(aes(x = department, y = order_hour_of_day)) + \n  geom_violin(bw = .6) +\n  theme(axis.text.x = element_text(angle = 90))\n\ninstacart %>%\n  mutate(department = forcats::fct_reorder(department, order_hour_of_day, IQR)) %>%\n  ggplot(aes(y = department, x = order_hour_of_day)) + geom_density_ridges()\n```\n\nBased on the violin and ridge plots above above, we see that the personal care department has the widest IQR, followed by the household department. The alcohol and canned goods departments have the narrowest IQRs. The shapes of the distributions are fairly consistent across departments, without major differences in the time of order. Small differences do exist, though: for example, alcohol is often ordered later in the afternoon (and less in the morning), while items from the babies department may be ordered earlier in the day (and over a wider range).\n\n### Problem 3\n\nLoad the NY NOAA data.\n\n```{r read_noaa}\nny_noaa = read_csv(\"../data/nynoaadat.zip\", \n                   col_types = \"cDiiidd\") %>%\n  separate(date, into = c(\"year\", \"month\", \"day\"))\n```\n\n\nWrite a short description of the dataset. How many observations are included? How many stations? How much missing data is there for tmax and snow? Does this vary by station?\n\n\n```{r noaa_summary, echo = FALSE}\nnmiss_tmax = ny_noaa %>%\n  mutate(na_tmax = is.na(tmax)) %>%\n  count(na_tmax) %>% filter(na_tmax == TRUE)\n\nnmiss_snow = ny_noaa %>%\n  mutate(na_snow = is.na(snow)) %>%\n  count(na_snow) %>% filter(na_snow == TRUE)\n```\n\nNotice that there is a lot of missing data in the variables related to temperature and precipitation. The dataset contains precipitation and temperature information for `r ny_noaa %>% summarize(n_distinct(id))` weather stations. There are a total of `r nrow(ny_noaa)` observations in the dataset, but only `r ny_noaa %>% na.omit() %>% nrow()` observations that have no missing values. For the `tmax` variable there are `r nmiss_tmax$n` missing values and for the `snow` variable there are `r nmiss_snow$n` missing values.\n\n******\n\nWhat is the year that contains the largest snowfall on a single day at any single weather station.\n\n```{r largest_snowfall}\nny_noaa %>% \n  group_by(year) %>% \n  summarize(max_snow = max(snow, na.rm = TRUE)) %>% \n  arrange(min_rank(desc(max_snow)))\n```\n\nBased on my findings, the largest snowfall on a single day at any single weather station was in 1983. Indeed, this [article](http://www.nytimes.com/1983/02/13/nyregion/20-inch-snowfall-paralyzes-much-mid-atlantic-area-25-die-sinking-off-virginia.html?pagewanted=all) from February 1983 describes the blizzard in some detail.\n\n******\n\nLimiting your data to observations with snowfall values greater than 0 and less than 100, make a ridge plot showing the distribution of snowfall values for each year. Comment on the recorded snowfall values – are they clustered around specific entries? If so, why?\n\n```{r snowfall_distribution}\nny_noaa %>% \n  filter(snow < 100, snow > 0) %>%\n  ggplot(aes(x = snow, y = year)) + geom_density_ridges()\n```\n\nThe snow fall values seem to be clustered around 25, 50, and 75, a trend which persists across years. Data are recorded in millimeters, and 1 inch is 25.4mm; this generally accounts for the pattern in these observations. Other values, especially those under 1 inch, are likely measured in fractions of an inch and coverted to mm. \n\n******\n\nMake a useful plot showing tmax against tmin. For these data, you might try a scatterplot, but it is unlikely to be “useful”.\n\n```{r tmax_vs_tmin}\nny_noaa %>% \n  ggplot(aes(x = tmin, y = tmax)) + \n  geom_hex()\n```\n\nThe hex plot above shows the bivariate distribution. While there is some variability, the majority of the data cluster tightly in the center of the distribution. In relatively rare cases, it seems that `tmax` is less than `tmin`, which raises questions about data recording and quality.\n\n******\n\nSeparate the date variable into year, month, and day variables. For each station and month, average across year to obtain the station-specific monthly average tmax. Make a spaghetti plot showing the average tmax curve for each station. Comment on your plot.\n\n```{r stations_tmax}\nny_noaa %>% \n  group_by(id, month) %>% \n  summarize(m = mean(tmax, na.rm = TRUE)) %>%\n  ggplot(aes(x = month, y = m, group = id)) + geom_path(alpha = .4)\n```\n\nThe plot above plots a curve for each station. The curve represents average `tmax` over the course of 12 months. While there is some variability across curves, most curves have a similar shape, with the coldest maximum temperatures in December through February and the peak maximum temperatures in July. \n",
    "created" : 1510072013765.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1058679982",
    "id" : "67559F93",
    "lastKnownWriteTime" : 1508356043,
    "last_content_update" : 1508356043,
    "path" : "~/Documents/data_science/p8105_hw3_ajg2202/p8105_hw3_ajg2202.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}